# -*- coding: utf-8 -*-
"""Ex1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8s2MJvuqnvMGTUfn1Y8TG0jZyv4P914
"""

pip install pyspark

from pyspark import SparkContext,SparkConf
import collections
import re
from operator import add
conf = SparkConf().setMaster("local").setAppName("words")
sc =SparkContext.getOrCreate(conf=conf)

print('File Path:')
filepath = input()
content = sc.textFile(filepath)

words = content.flatMap(lambda line: line.split(" "))
# print(words.collect())
# rdd = sc.parallelize(words.collect())
counts = words.map(lambda word : (word,1))

def map_phase(x):
    x = re.sub('--', ' ', x)
    x = re.sub("'", '', x)
    return re.sub('[?!@#$\'",.;:()]', '', x).lower().split(' ')

def pass_filter(x):
    return (len(x) > 0 or x != " " or x != None)

counts = content.flatMap(map_phase) \
                  .map(lambda x: (x, 1)) \
                  .reduceByKey(add) \
                  .filter(pass_filter)

print('Số Từ:')
number = int(input())
output = counts.map(lambda x:  (x[1], x[0])).sortByKey(False).take(number)
for (count, word) in output:
  print("%i: %s" % (count, word))